{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Educative-NLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M78YgWec-kYX",
        "colab_type": "text"
      },
      "source": [
        "## Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gjd5AzPSK0eq",
        "colab_type": "code",
        "outputId": "6162c6ac-a2b4-4b57-8fef-10e5e100adf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "text_corpus = ['bob ate apples, and pears', 'fred ate apples!']\n",
        "new_texts = ['bob ate pears', 'fred ate pears']"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3, 1, 5], [6, 1, 5]]\n",
            "{'ate': 1, 'apples': 2, 'bob': 3, 'and': 4, 'pears': 5, 'fred': 6}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpfulpcL7GmD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(text_corpus)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJp012ll7Ddv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(tokenizer.texts_to_sequences(new_texts))\n",
        "print(tokenizer.word_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZ9XAdwl6tpR",
        "colab_type": "text"
      },
      "source": [
        "By default, the `Tokenizer` filters out any punctuation and white space. You can specify custom filtering with the `filters` parameter. The parameter takes in a string, where each character in the string is filtered out.\n",
        "\n",
        "When a new text contains words not in the corpus vocabulary, those words are known as out-of-vocabulary (OOV) words. The `texts_to_sequences` automatically filters out all OOV words. However, if we want to specify each OOV word with a special vocabulary token (e.g. `'OOV'`), we can initialize the `Tokenizer` with the `oov_token` parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVB0xIqC6wH3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='OOV')\n",
        "tokenizer.fit_on_texts(text_corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pWcOfdR7PFu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "63183fab-9471-4021-98aa-bef905044d20"
      },
      "source": [
        "print(tokenizer.texts_to_sequences(['bob ate bacon']))\n",
        "print(tokenizer.word_index)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[4, 2, 1]]\n",
            "{'OOV': 1, 'ate': 2, 'apples': 3, 'bob': 4, 'and': 5, 'pears': 6, 'fred': 7}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tl_oVa4q8XhT",
        "colab_type": "text"
      },
      "source": [
        "The `num_words` parameter lets us specify the maximum number of vocabulary words to use. For example, if we set `num_words=100` when initializing the `Tokenizer`, it will only use the 100 most frequent words in the vocabulary and filter out the remaining vocabulary words. This can be useful when the text corpus is large and you need to limit the vocabulary size to increase training speed or prevent overfitting on infrequent words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdsqYMB38jCZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=2)\n",
        "text_corpus = ['bob ate apples, and pears', 'fred ate apples!']\n",
        "tokenizer.fit_on_texts(text_corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdXt88d68oW1",
        "colab_type": "text"
      },
      "source": [
        "The two most common words are 'ate' and 'apples'. The tokenizer will filter out all other words for the sentence 'bob ate pears', only 'ate' will be kept since 'ate' maps to an integer ID of 1, the only value  in the token sequence will be 1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wf1Pu8Yf8wU7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8f0c1793-4ed0-4ee6-e3b8-1727ca356f2b"
      },
      "source": [
        "print(tokenizer.texts_to_sequences(['bob ate pears']))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}